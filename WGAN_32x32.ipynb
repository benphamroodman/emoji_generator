{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d3aabb6d-f825-43ef-b69e-34625a683beb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3aabb6d-f825-43ef-b69e-34625a683beb",
        "outputId": "011d3a4f-426c-4446-cadb-5f0fd89d8b72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Mounted at /content/drive\n",
            "Cuda available:  True\n",
            "Using Cuda:      True\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "#%matplotlib inline\n",
        "%pip install torch\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "colab_path = \"/content/drive/MyDrive/NTU_AI/\" #CHANGE THIS\n",
        "\n",
        "\n",
        "default_dtype = torch.float32\n",
        "torch.set_default_dtype(default_dtype)\n",
        "\n",
        "# Initialize CUDA\n",
        "use_cuda = True\n",
        "\n",
        "cuda_available = torch.cuda.is_available()\n",
        "USE_CUDA = use_cuda and cuda_available\n",
        "\n",
        "print(\"Cuda available: \", cuda_available)\n",
        "print(\"Using Cuda:     \", USE_CUDA)\n",
        "\n",
        "if cuda_available:\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    cuda_device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b2e6e374-5d12-4a26-9ca2-94eba243fcc0",
      "metadata": {
        "id": "b2e6e374-5d12-4a26-9ca2-94eba243fcc0"
      },
      "outputs": [],
      "source": [
        "# Root directory for dataset\n",
        "dataroot = \"processed_32x32\"\n",
        "if IN_COLAB:\n",
        "    dataroot = colab_path + dataroot\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_w = 32\n",
        "image_h = 32\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# nz = 100\n",
        "# ngf = 64\n",
        "nz = 100\n",
        "ngf = 64\n",
        "\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 5\n",
        "\n",
        "# Beta1 hyperparameter for Adam optimizers\n",
        "beta1 = 0.5\n",
        "# Learning rate for optimizers\n",
        "lr = 5e-5\n",
        "\n",
        "# For WGAN:\n",
        "critic_iterations = 5\n",
        "weight_clip = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c5863766-61f6-4d4f-b02e-76a9c04192f2",
      "metadata": {
        "id": "c5863766-61f6-4d4f-b02e-76a9c04192f2"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "onlyfiles = [f for f in listdir(dataroot) if isfile(join(dataroot, f))]\n",
        "\n",
        "sample_count = 0\n",
        "for file in onlyfiles:\n",
        "    if \".png\" in file:\n",
        "        sample_count += 1\n",
        "\n",
        "dataset = np.zeros((sample_count, image_h, image_w, nc), dtype = np.float32)\n",
        "i = 0\n",
        "\n",
        "for file in onlyfiles:\n",
        "    if \".png\" in file:\n",
        "        img = Image.open(dataroot + \"/\" + file)\n",
        "\n",
        "        dataset[i, :, :, :] = np.array(img) / 255\n",
        "        # plt.imshow(dataset[i])\n",
        "        # plt.show()\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "649b4a99-94ae-4d36-b2e5-46731d9fbf78",
      "metadata": {
        "id": "649b4a99-94ae-4d36-b2e5-46731d9fbf78"
      },
      "outputs": [],
      "source": [
        "# custom weights initialization called on ``netG`` and ``netD``\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "# Generator Code\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 4, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*8) x 4 x 4``\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*4) x 8 x 8``\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*2) x 16 x 16``\n",
        "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. ``(nc) x 32 x 32``\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is ``(nc) x 32 x 32``\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf) x 16 x 16``\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*4) x 8 x 8``\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*8) x 4 x 4``\n",
        "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2cdbd94c-9c67-464c-9df3-b12fc9bb0f86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cdbd94c-9c67-464c-9df3-b12fc9bb0f86",
        "outputId": "2dc087b6-5ca6-41e0-dcdd-b5131698dc44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "    (9): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the generator\n",
        "netG = Generator()\n",
        "# Create the Discriminator\n",
        "critic = Discriminator()\n",
        "\n",
        "if USE_CUDA:\n",
        "    netG = netG.cuda()\n",
        "    critic = critic.cuda()\n",
        "\n",
        "netG.apply(weights_init)\n",
        "critic.apply(weights_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6da49e59-e027-4118-a56c-009d2dadd0c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6da49e59-e027-4118-a56c-009d2dadd0c3",
        "outputId": "436b1676-8148-40cc-b44a-5e4599e68a69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4724, 32, 32, 3)\n",
            "(4724, 3, 32, 32)\n"
          ]
        }
      ],
      "source": [
        "# Initialize the ``BCELoss`` function, comment out for WGAN/uncomment for GAN\n",
        "# criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "\n",
        "noise_h = 1\n",
        "noise_w = 1\n",
        "\n",
        "fixed_noise = torch.randn(64, nz, noise_h, noise_w)\n",
        "if USE_CUDA:\n",
        "    fixed_noise = fixed_noise.cuda()\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "opt_critic = optim.RMSprop(critic.parameters(), lr=lr) # Formerly optimizerD\n",
        "optimizerG = optim.RMSprop(netG.parameters(), lr=lr)\n",
        "\n",
        "print(dataset.shape)\n",
        "dataset_flipped = np.moveaxis(dataset, -1, 1)\n",
        "print(dataset_flipped.shape)\n",
        "\n",
        "dataset_t = torch.from_numpy(dataset_flipped).to(dtype = default_dtype)\n",
        "if USE_CUDA:\n",
        "    dataset_t = dataset_t.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8d7c562b-cafa-4855-827f-94dcedba8872",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8d7c562b-cafa-4855-827f-94dcedba8872",
        "outputId": "e52980fa-59df-48c6-d998-ca9ca89f1d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training Loop...\n",
            "[0/500][0/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][15/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][30/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][45/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][60/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][75/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][90/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][105/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][120/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][135/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][150/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][165/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][180/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][195/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][210/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][225/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][240/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][255/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][270/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][285/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][300/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][315/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][330/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][345/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][360/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][375/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][390/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][405/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][420/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][435/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][450/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][465/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][480/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][495/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][510/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][525/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][540/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][555/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][570/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][585/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][600/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][615/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][630/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][645/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][660/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][675/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][690/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][705/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][720/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][735/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][750/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][765/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][780/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][795/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][810/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][825/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][840/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][855/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][870/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][885/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][900/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][915/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][930/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][945/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][960/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][975/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][990/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][1005/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][1020/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][1035/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][1050/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][1065/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][1080/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][1095/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][1110/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][1125/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][1140/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][1155/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[0/500][1170/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/500][0/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][15/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][30/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][45/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][60/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][75/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][90/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][105/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][120/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][135/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][150/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][165/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][180/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][195/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][210/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][225/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][240/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][255/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][270/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][285/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][300/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][315/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][330/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][345/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][360/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][375/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][390/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][405/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][420/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][435/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][450/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][465/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][480/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][495/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][510/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][525/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][540/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][555/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][570/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][585/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][600/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][615/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][630/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][645/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][660/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][675/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][690/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][705/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][720/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][735/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][750/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][765/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][780/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][795/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][810/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][825/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][840/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][855/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][870/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][885/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][900/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n",
            "[1/500][915/1181]\tLoss_D: 0.0000\tLoss_G: 0.0000\tD(x): 0.0000\tD(G(z)): 0.0000 / 0.0000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-4875d8364b6c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0;31m# For WGAN- someone confirm for me that real_cpu is the right argument here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m           \u001b[0mcritic_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m           \u001b[0mcritic_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mloss_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_real\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-af1e71832785>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2507\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2509\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2510\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2511\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "batch_size = 4\n",
        "num_epochs = 500\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    batch_count = int(np.ceil(sample_count / batch_size))\n",
        "    permutation = np.random.permutation(sample_count)\n",
        "    for batch_idx in range(batch_count):\n",
        "        print(\"epoch: {} \\t {}/{} batch\".format(epoch, batch_idx, batch_count), end=\"\\r\")\n",
        "\n",
        "        batch_start = batch_idx * batch_size\n",
        "        batch_end = min(sample_count, (batch_idx + 1) * batch_size)\n",
        "        batch_indeces = permutation[batch_start:batch_end]\n",
        "        real_batch_size = batch_end - batch_start\n",
        "\n",
        "        data = dataset_t[batch_indeces]\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch, uncomment for GAN\n",
        "        # netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data\n",
        "        # label = torch.full((real_batch_size,), real_label, dtype=torch.float)\n",
        "        # if USE_CUDA:\n",
        "        #     label = label.cuda()\n",
        "        # # Forward pass real batch through D\n",
        "        # output = netD(real_cpu).view(-1)\n",
        "        # # Calculate loss on all-real batch\n",
        "        # errD_real = criterion(output, label)\n",
        "        # # Calculate gradients for D in backward pass\n",
        "        # errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        for _ in range(critic_iterations):\n",
        "          ## Train with all-fake batch\n",
        "          # Generate batch of latent vectors\n",
        "          noise = torch.randn(real_batch_size, nz, noise_h, noise_w)\n",
        "          if USE_CUDA:\n",
        "              noise = noise.cuda()\n",
        "          # Generate fake image batch with G\n",
        "          fake = netG(noise)\n",
        "\n",
        "          # For WGAN- someone confirm for me that real_cpu is the right argument here.\n",
        "          critic_real = critic(real_cpu).reshape(-1)\n",
        "          critic_fake = critic(fake).reshape(-1)\n",
        "          loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
        "          critic.zero_grad()\n",
        "          loss_critic.backward(retain_graph=True)\n",
        "          opt_critic.step()\n",
        "\n",
        "          for p in critic.parameters(): p.data.clamp_(- weight_clip, weight_clip)\n",
        "\n",
        "        # Train generator: min -E[critic(gen_fake)]\n",
        "        output = critic(fake).reshape(-1)\n",
        "        loss_gen = - torch.mean(output)\n",
        "        netG.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        # label.fill_(fake_label)\n",
        "        # # Classify all fake batch with D\n",
        "        # output = netD(fake.detach()).view(-1)\n",
        "        # # Calculate D's loss on the all-fake batch\n",
        "        # errD_fake = criterion(output, label)\n",
        "        # # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        # errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # # Compute error of D as sum over the fake and the real batches\n",
        "        # errD = errD_real + errD_fake\n",
        "        # # Update D\n",
        "        # optimizerD.step()\n",
        "\n",
        "        # ############################\n",
        "        # # (2) Update G network: maximize log(D(G(z)))\n",
        "        # ###########################\n",
        "        # netG.zero_grad()\n",
        "        # label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        # output = netD(fake).view(-1)\n",
        "        # # Calculate G's loss based on this output\n",
        "        # errG = criterion(output, label)\n",
        "        # # Calculate gradients for G\n",
        "        # errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # # Update G\n",
        "        # optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if batch_idx % 15 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, batch_idx, batch_count, 0, 0, 0, 0, 0))\n",
        "            # Replaced errD.item(), errG.item(), D_x, D_G_z1, and D_G_z2 with 0s in the print statement for WGAN.\n",
        "\n",
        "        # # Save Losses for plotting later\n",
        "        # G_losses.append(errG.item())\n",
        "        # D_losses.append(errD.item())\n",
        "\n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (batch_idx == batch_count - 1 ):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(fake.numpy())\n",
        "            index = 0\n",
        "            img = fake.numpy()[0]\n",
        "            img = np.moveaxis(img, 0, -1)\n",
        "            img = (img - img.min()) / (img.max() - img.min())\n",
        "\n",
        "            plt.imshow(img)\n",
        "            plt.show()\n",
        "\n",
        "        iters += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4521732d-081d-468f-9052-65d87e373b91",
      "metadata": {
        "id": "4521732d-081d-468f-9052-65d87e373b91"
      },
      "outputs": [],
      "source": [
        "for img in img_list[150]:\n",
        "    img = np.moveaxis(img, 0, -1)\n",
        "    img = (img - img.min()) / (img.max() - img.min())\n",
        "\n",
        "    print(img.shape)\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96eb2e57-3666-4b33-8ca6-47abfed094e4",
      "metadata": {
        "id": "96eb2e57-3666-4b33-8ca6-47abfed094e4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
