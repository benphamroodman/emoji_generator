{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3aabb6d-f825-43ef-b69e-34625a683beb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-03T23:11:42.534412Z",
     "end_time": "2024-06-03T23:11:42.538695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  False\n",
      "Using Cuda:      False\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "default_dtype = torch.float32\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Initialize CUDA\n",
    "use_cuda = False\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "USE_CUDA = use_cuda and cuda_available\n",
    "\n",
    "print(\"Cuda available: \", cuda_available)\n",
    "print(\"Using Cuda:     \", USE_CUDA)\n",
    "\n",
    "if cuda_available:\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    cuda_device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e6e374-5d12-4a26-9ca2-94eba243fcc0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-03T23:11:45.198444Z",
     "end_time": "2024-06-03T23:11:45.201166Z"
    }
   },
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"discord_images\"\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 512\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_w = 463\n",
    "image_h = 109\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mo\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5863766-61f6-4d4f-b02e-76a9c04192f2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-03T22:54:26.555710Z",
     "end_time": "2024-06-03T22:58:14.103614Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulfeichten/anaconda3/lib/python3.10/site-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "onlyfiles = [f for f in listdir(dataroot) if isfile(join(dataroot, f))]\n",
    "\n",
    "sample_count = 0\n",
    "for file in onlyfiles:\n",
    "    if \".png\" in file:\n",
    "        sample_count += 1\n",
    "\n",
    "new_width = 512\n",
    "new_height = 128\n",
    "dataset = np.zeros((sample_count, new_height, new_width, 3), dtype = np.float32)\n",
    "i = 0\n",
    "\n",
    "for file in onlyfiles:\n",
    "    if \".png\" in file:\n",
    "        img = Image.open(dataroot + \"/\" + file).resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        dataset[i, :, :, :] = np.array(img) / 255\n",
    "        # plt.imshow(dataset[i])\n",
    "        # plt.show()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "649b4a99-94ae-4d36-b2e5-46731d9fbf78",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-03T23:11:50.925169Z",
     "end_time": "2024-06-03T23:11:50.933758Z"
    }
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on ``netG`` and ``netD``\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*8) x 4 x 4``\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*4) x 8 x 8``\n",
    "            nn.ConvTranspose2d( ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*2) x 16 x 16``\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf) x 32 x 32``\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(nc) x 64 x 64``\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, ngpu):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.ngpu = ngpu\n",
    "        \n",
    "#         self.step0  = nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False)\n",
    "#         self.step1  = nn.BatchNorm2d(ngf * 8)\n",
    "#         self.step2  = nn.ReLU(True)\n",
    "#         self.step3  = nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False)\n",
    "#         self.step4  = nn.BatchNorm2d(ngf * 4)\n",
    "#         self.step5  = nn.ReLU(True)\n",
    "#         self.step6  = nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False)\n",
    "#         self.step7  = nn.BatchNorm2d(ngf * 2)\n",
    "#         self.step8  = nn.ReLU(True)\n",
    "#         self.step9  = nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False)\n",
    "#         self.step10 = nn.BatchNorm2d(ngf)\n",
    "#         self.step11 = nn.ReLU(True)\n",
    "#         self.step12 = nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False)\n",
    "#         self.step13 = nn.Tanh()\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         x = input\n",
    "#         print(x.shape)\n",
    "#         x = self.step0 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.step1 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.step2 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.step3 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.step4 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.step5 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.step6 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.step7 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.step8 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.step9 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.step10(x)\n",
    "#         print(x.shape)\n",
    "#         x = self.step11(x)\n",
    "#         print(x.shape)\n",
    "#         x = self.step12(x)\n",
    "#         print(x.shape)\n",
    "#         x = self.step13(x)\n",
    "#         print(x.shape)\n",
    "#         return x\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, ngpu):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.ngpu = ngpu\n",
    "#         self.main = nn.Sequential(\n",
    "#             # input is ``(nc) x 64 x 64``\n",
    "#             nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. ``(ndf) x 32 x 32``\n",
    "#             nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ndf * 2),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. ``(ndf*2) x 16 x 16``\n",
    "#             nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ndf * 4),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. ``(ndf*4) x 8 x 8``\n",
    "#             nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "#             nn.BatchNorm2d(ndf * 8),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             # state size. ``(ndf*8) x 4 x 4``\n",
    "#             nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         return self.main(input)\n",
    "        \n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, ngpu):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.ngpu = ngpu\n",
    "#         self.conv1 = nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)\n",
    "#         self.act1  = nn.LeakyReLU(0.2, inplace=True)\n",
    "#         self.conv2 = nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)\n",
    "#         self.bn1   = nn.BatchNorm2d(ndf * 2)\n",
    "#         self.relu2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "#         self.conv3 = nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)\n",
    "#         self.bn2   = nn.BatchNorm2d(ndf * 4)\n",
    "#         self.relu3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "#         self.conv4 = nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)\n",
    "#         self.bn3   = nn.BatchNorm2d(ndf * 8)\n",
    "#         self.relu4 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "#         self.conv5 = nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False)\n",
    "#         self.fc = nn.Linear(29*5, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "#     def forward(self, input):\n",
    "#         x = input\n",
    "#         print(x.shape)\n",
    "#         x = self.conv1(x)\n",
    "#         print(x.shape)\n",
    "#         x = self.act1  (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.conv2 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.bn1   (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.relu2 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.conv3 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.bn2   (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.relu3 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.conv4 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.bn3   (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.relu4 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.conv5 (x)\n",
    "#         print(x.shape)\n",
    "#         x = self.fc(x.reshape(x.shape[0], -1))\n",
    "#         output = self.sigmoid(x)\n",
    "        \n",
    "#         print(output.shape)\n",
    "#         return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.conv1 = nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)\n",
    "        self.act1  = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(ndf * 2)\n",
    "        self.relu2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(ndf * 4)\n",
    "        self.relu3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)\n",
    "        self.bn3   = nn.BatchNorm2d(ndf * 8)\n",
    "        self.relu4 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False)\n",
    "        self.fc = nn.Linear(29*5, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1  (x)\n",
    "        x = self.conv2 (x)\n",
    "        x = self.bn1   (x)\n",
    "        x = self.relu2 (x)\n",
    "        x = self.conv3 (x)\n",
    "        x = self.bn2   (x)\n",
    "        x = self.relu3 (x)\n",
    "        x = self.conv4 (x)\n",
    "        x = self.bn3   (x)\n",
    "        x = self.relu4 (x)\n",
    "        x = self.conv5 (x)\n",
    "        x = self.fc(x.reshape(x.shape[0], -1))\n",
    "        output = self.sigmoid(x)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cdbd94c-9c67-464c-9df3-b12fc9bb0f86",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-03T23:11:53.015066Z",
     "end_time": "2024-06-03T23:11:53.364473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Discriminator(\n  (conv1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n  (act1): LeakyReLU(negative_slope=0.2, inplace=True)\n  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu2): LeakyReLU(negative_slope=0.2, inplace=True)\n  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu3): LeakyReLU(negative_slope=0.2, inplace=True)\n  (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu4): LeakyReLU(negative_slope=0.2, inplace=True)\n  (conv5): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n  (fc): Linear(in_features=145, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the generator\n",
    "netG = Generator(ngpu)\n",
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu)\n",
    "\n",
    "if USE_CUDA:\n",
    "    netG = netG.cuda()\n",
    "    netD = netD.cuda()\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da49e59-e027-4118-a56c-009d2dadd0c3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-03T23:11:57.643155Z",
     "end_time": "2024-06-03T23:11:57.662306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78763, 128, 512, 3)\n",
      "(78763, 3, 128, 512)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ``BCELoss`` function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "\n",
    "noise_h = int((new_height / 16) - 3)\n",
    "noise_w = int((new_width / 16) - 3)\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, noise_h, noise_w)\n",
    "if USE_CUDA:\n",
    "    fixed_noise = fixed_noise.cuda()\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "print(dataset.shape)\n",
    "dataset_flipped = np.moveaxis(dataset, -1, 1)\n",
    "print(dataset_flipped.shape)\n",
    "\n",
    "dataset_t = torch.from_numpy(dataset_flipped).to(dtype = default_dtype)\n",
    "if USE_CUDA:\n",
    "    dataset_t = dataset_t.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c562b-cafa-4855-827f-94dcedba8872",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-03T22:28:05.811296Z",
     "end_time": "2024-06-03T22:29:01.870823Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "test1: 0 \t 0/154 batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "num_epochs = 100\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    batch_count = int(np.ceil(sample_count / batch_size))\n",
    "    permutation = np.random.permutation(sample_count)\n",
    "    for batch_idx in range(batch_count):\n",
    "        print(\"epoch: {} \\t {}/{} batch\".format(epoch, batch_idx, batch_count), end=\"\\r\")\n",
    "        \n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = min(sample_count, (batch_idx + 1) * batch_size)\n",
    "        batch_indeces = permutation[batch_start:batch_end]\n",
    "        real_batch_size = batch_end - batch_start\n",
    "        \n",
    "        data = dataset_t[batch_indeces]\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data\n",
    "        print('test1')\n",
    "        label = torch.full((real_batch_size,), real_label, dtype=torch.float)\n",
    "        if USE_CUDA:\n",
    "            label = label.cuda()\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        print('test2')\n",
    "    # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        print('test3')\n",
    "\n",
    "    ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(real_batch_size, nz, noise_h, noise_w)\n",
    "        if USE_CUDA:\n",
    "            noise = noise.cuda()\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        print('test4')\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        print('test5')\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        print('test6')\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        print('test7')\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if batch_idx % 15 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, batch_idx, batch_count, errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        print('test8')\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (batch_idx == batch_count - 1 ):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(fake.numpy())\n",
    "            index = 0\n",
    "            img = fake.numpy()[0]\n",
    "            img = np.moveaxis(img, 0, -1)\n",
    "            # img = np.swapaxes(img, 0, 2)\n",
    "            # img = np.swapaxes(img, 0, 1)\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "            \n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521732d-081d-468f-9052-65d87e373b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs in img_list:\n",
    "    index = 0\n",
    "    img = imgs[0]\n",
    "    img = np.swapaxes(img, 0, 2)\n",
    "    img = np.swapaxes(img, 0, 1)\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    \n",
    "    print(img.shape)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb2e57-3666-4b33-8ca6-47abfed094e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
