{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3aabb6d-f825-43ef-b69e-34625a683beb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-27T14:41:02.001725Z",
     "end_time": "2024-05-27T14:41:02.922424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  False\n",
      "Using Cuda:      False\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    \n",
    "colab_path = \"/content/drive/MyDrive/Colab Notebooks/NTU_AI/\" #CHANGE THIS\n",
    "\n",
    "\n",
    "default_dtype = torch.float32\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "# Initialize CUDA\n",
    "use_cuda = True\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "USE_CUDA = use_cuda and cuda_available\n",
    "\n",
    "print(\"Cuda available: \", cuda_available)\n",
    "print(\"Using Cuda:     \", USE_CUDA)\n",
    "\n",
    "if cuda_available:\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    cuda_device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e6e374-5d12-4a26-9ca2-94eba243fcc0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-27T14:43:19.731855Z",
     "end_time": "2024-05-27T14:43:19.735418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"processed_32x32\"\n",
    "if IN_COLAB:\n",
    "    dataroot = colab_path + dataroot\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_w = 32\n",
    "image_h = 32\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# nz = 100\n",
    "# ngf = 64\n",
    "nz = 100\n",
    "ngf = 64\n",
    "\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5863766-61f6-4d4f-b02e-76a9c04192f2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-27T14:43:25.492085Z",
     "end_time": "2024-05-27T14:43:26.693620Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "onlyfiles = [f for f in listdir(dataroot) if isfile(join(dataroot, f))]\n",
    "\n",
    "sample_count = 0\n",
    "for file in onlyfiles:\n",
    "    if \".png\" in file:\n",
    "        sample_count += 1\n",
    "\n",
    "dataset = np.zeros((sample_count, image_h, image_w, nc), dtype = np.float32)\n",
    "i = 0\n",
    "\n",
    "for file in onlyfiles:\n",
    "    if \".png\" in file:\n",
    "        img = Image.open(dataroot + \"/\" + file)\n",
    "\n",
    "        dataset[i, :, :, :] = np.array(img) / 255\n",
    "        # plt.imshow(dataset[i])\n",
    "        # plt.show()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "649b4a99-94ae-4d36-b2e5-46731d9fbf78",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-27T14:43:29.053112Z",
     "end_time": "2024-05-27T14:43:29.153557Z"
    }
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on ``netG`` and ``netD``\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# Generator Code\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, inputs, filters, stride):\n",
    "        super(Block, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 4, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*8) x 4 x 4``\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*4) x 8 x 8``\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. ``(ngf*2) x 16 x 16``\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. ``(nc) x 32 x 32``\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "        \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is ``(nc) x 32 x 32``\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf) x 16 x 16``\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*4) x 8 x 8``\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. ``(ndf*8) x 4 x 4``\n",
    "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cdbd94c-9c67-464c-9df3-b12fc9bb0f86",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-27T14:43:34.615935Z",
     "end_time": "2024-05-27T14:43:34.723878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Discriminator(\n  (main): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (9): Sigmoid()\n  )\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the generator\n",
    "netG = Generator()\n",
    "# Create the Discriminator\n",
    "netD = Discriminator()\n",
    "\n",
    "if USE_CUDA:\n",
    "    netG = netG.cuda()\n",
    "    netD = netD.cuda()\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da49e59-e027-4118-a56c-009d2dadd0c3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-27T14:43:38.385557Z",
     "end_time": "2024-05-27T14:43:38.496084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4724, 32, 32, 3)\n",
      "(4724, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ``BCELoss`` function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "\n",
    "noise_h = 1\n",
    "noise_w = 1\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, noise_h, noise_w)\n",
    "if USE_CUDA:\n",
    "    fixed_noise = fixed_noise.cuda()\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "print(dataset.shape)\n",
    "dataset_flipped = np.moveaxis(dataset, -1, 1)\n",
    "print(dataset_flipped.shape)\n",
    "\n",
    "dataset_t = torch.from_numpy(dataset_flipped).to(dtype = default_dtype)\n",
    "if USE_CUDA:\n",
    "    dataset_t = dataset_t.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d7c562b-cafa-4855-827f-94dcedba8872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[0/500][0/1181]\tLoss_D: 1.5883\tLoss_G: 1.6264\tD(x): 0.4680\tD(G(z)): 0.5285 / 0.1989\n",
      "epoch: 0 \t 1/1181 batch\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/500][15/1181]\tLoss_D: 0.3611\tLoss_G: 2.6900\tD(x): 0.9740\tD(G(z)): 0.2613 / 0.0742\n",
      "[0/500][30/1181]\tLoss_D: 0.2206\tLoss_G: 3.1101\tD(x): 0.9397\tD(G(z)): 0.1439 / 0.0452\n",
      "[0/500][45/1181]\tLoss_D: 0.2080\tLoss_G: 3.3413\tD(x): 0.9770\tD(G(z)): 0.1654 / 0.0377\n",
      "[0/500][60/1181]\tLoss_D: 0.2077\tLoss_G: 3.1534\tD(x): 0.9568\tD(G(z)): 0.1470 / 0.0439\n",
      "[0/500][75/1181]\tLoss_D: 0.1645\tLoss_G: 3.9103\tD(x): 0.8853\tD(G(z)): 0.0394 / 0.0217\n",
      "[0/500][90/1181]\tLoss_D: 0.0846\tLoss_G: 4.4281\tD(x): 0.9917\tD(G(z)): 0.0706 / 0.0151\n",
      "[0/500][105/1181]\tLoss_D: 0.3492\tLoss_G: 4.5424\tD(x): 0.9834\tD(G(z)): 0.2749 / 0.0110\n",
      "[0/500][120/1181]\tLoss_D: 0.5064\tLoss_G: 2.8363\tD(x): 0.6829\tD(G(z)): 0.0395 / 0.0594\n",
      "[0/500][135/1181]\tLoss_D: 3.3213\tLoss_G: 0.9284\tD(x): 0.0596\tD(G(z)): 0.0434 / 0.4124\n",
      "[0/500][150/1181]\tLoss_D: 0.2001\tLoss_G: 3.2126\tD(x): 0.9025\tD(G(z)): 0.0895 / 0.0406\n",
      "[0/500][165/1181]\tLoss_D: 0.0602\tLoss_G: 4.3417\tD(x): 0.9669\tD(G(z)): 0.0259 / 0.0139\n",
      "[0/500][180/1181]\tLoss_D: 0.0768\tLoss_G: 3.8152\tD(x): 0.9951\tD(G(z)): 0.0692 / 0.0232\n",
      "[0/500][195/1181]\tLoss_D: 0.3476\tLoss_G: 2.8079\tD(x): 0.7468\tD(G(z)): 0.0304 / 0.0643\n",
      "[0/500][210/1181]\tLoss_D: 0.2827\tLoss_G: 3.5364\tD(x): 0.8165\tD(G(z)): 0.0184 / 0.0314\n",
      "[0/500][225/1181]\tLoss_D: 0.4050\tLoss_G: 5.7035\tD(x): 0.9956\tD(G(z)): 0.3274 / 0.0035\n",
      "[0/500][240/1181]\tLoss_D: 0.1005\tLoss_G: 4.6786\tD(x): 0.9252\tD(G(z)): 0.0198 / 0.0118\n",
      "[0/500][255/1181]\tLoss_D: 0.0752\tLoss_G: 4.4612\tD(x): 0.9557\tD(G(z)): 0.0264 / 0.0157\n",
      "[0/500][270/1181]\tLoss_D: 0.0627\tLoss_G: 4.4709\tD(x): 0.9796\tD(G(z)): 0.0406 / 0.0134\n",
      "[0/500][285/1181]\tLoss_D: 0.0772\tLoss_G: 4.3613\tD(x): 0.9816\tD(G(z)): 0.0561 / 0.0141\n",
      "[0/500][300/1181]\tLoss_D: 0.7708\tLoss_G: 1.7096\tD(x): 0.5771\tD(G(z)): 0.0236 / 0.1872\n",
      "[0/500][315/1181]\tLoss_D: 0.1308\tLoss_G: 4.2668\tD(x): 0.9523\tD(G(z)): 0.0765 / 0.0166\n",
      "[0/500][330/1181]\tLoss_D: 0.0473\tLoss_G: 5.4295\tD(x): 0.9596\tD(G(z)): 0.0050 / 0.0046\n",
      "[0/500][345/1181]\tLoss_D: 0.1417\tLoss_G: 3.6358\tD(x): 0.9326\tD(G(z)): 0.0641 / 0.0275\n",
      "[0/500][360/1181]\tLoss_D: 0.0398\tLoss_G: 4.7139\tD(x): 0.9964\tD(G(z)): 0.0351 / 0.0116\n",
      "[0/500][375/1181]\tLoss_D: 0.0320\tLoss_G: 4.2939\tD(x): 0.9922\tD(G(z)): 0.0239 / 0.0144\n",
      "[0/500][390/1181]\tLoss_D: 0.0682\tLoss_G: 4.6484\tD(x): 0.9833\tD(G(z)): 0.0499 / 0.0097\n",
      "[0/500][405/1181]\tLoss_D: 0.0633\tLoss_G: 5.4491\tD(x): 0.9784\tD(G(z)): 0.0401 / 0.0046\n",
      "[0/500][420/1181]\tLoss_D: 0.2106\tLoss_G: 3.6707\tD(x): 0.8985\tD(G(z)): 0.0904 / 0.0257\n",
      "[0/500][435/1181]\tLoss_D: 0.3431\tLoss_G: 5.9889\tD(x): 0.7406\tD(G(z)): 0.0020 / 0.0032\n",
      "[0/500][450/1181]\tLoss_D: 0.2296\tLoss_G: 7.2749\tD(x): 0.8912\tD(G(z)): 0.0893 / 0.0073\n",
      "[0/500][465/1181]\tLoss_D: 0.4341\tLoss_G: 3.7618\tD(x): 0.6622\tD(G(z)): 0.0087 / 0.0316\n",
      "[0/500][480/1181]\tLoss_D: 0.0974\tLoss_G: 5.3384\tD(x): 0.9206\tD(G(z)): 0.0119 / 0.0058\n",
      "[0/500][495/1181]\tLoss_D: 0.5048\tLoss_G: 3.0941\tD(x): 0.9268\tD(G(z)): 0.3464 / 0.0478\n",
      "[0/500][510/1181]\tLoss_D: 0.1204\tLoss_G: 2.8004\tD(x): 0.9839\tD(G(z)): 0.0975 / 0.0668\n",
      "[0/500][525/1181]\tLoss_D: 0.4902\tLoss_G: 3.4497\tD(x): 0.9623\tD(G(z)): 0.3552 / 0.0328\n",
      "[0/500][540/1181]\tLoss_D: 1.0812\tLoss_G: 0.4096\tD(x): 0.4206\tD(G(z)): 0.1742 / 0.6722\n",
      "[0/500][555/1181]\tLoss_D: 0.7961\tLoss_G: 3.3812\tD(x): 0.6998\tD(G(z)): 0.2887 / 0.0374\n",
      "[0/500][570/1181]\tLoss_D: 0.4135\tLoss_G: 3.3230\tD(x): 0.8053\tD(G(z)): 0.1510 / 0.0418\n",
      "[0/500][585/1181]\tLoss_D: 0.1722\tLoss_G: 2.6164\tD(x): 0.9078\tD(G(z)): 0.0709 / 0.0803\n",
      "[0/500][600/1181]\tLoss_D: 0.2038\tLoss_G: 4.0088\tD(x): 0.8990\tD(G(z)): 0.0798 / 0.0197\n",
      "[0/500][615/1181]\tLoss_D: 0.3792\tLoss_G: 2.7949\tD(x): 0.9273\tD(G(z)): 0.2564 / 0.0638\n",
      "[0/500][630/1181]\tLoss_D: 0.3959\tLoss_G: 2.0038\tD(x): 0.8321\tD(G(z)): 0.1838 / 0.1406\n",
      "[0/500][645/1181]\tLoss_D: 0.6225\tLoss_G: 3.0490\tD(x): 0.8674\tD(G(z)): 0.3443 / 0.0582\n",
      "[0/500][660/1181]\tLoss_D: 0.8360\tLoss_G: 2.2377\tD(x): 0.5605\tD(G(z)): 0.1382 / 0.1335\n",
      "[0/500][675/1181]\tLoss_D: 0.8721\tLoss_G: 2.3790\tD(x): 0.7107\tD(G(z)): 0.2779 / 0.1111\n",
      "[0/500][690/1181]\tLoss_D: 0.5671\tLoss_G: 2.1913\tD(x): 0.7645\tD(G(z)): 0.2277 / 0.1260\n",
      "[0/500][705/1181]\tLoss_D: 0.6349\tLoss_G: 2.2072\tD(x): 0.5878\tD(G(z)): 0.0912 / 0.1170\n",
      "[0/500][720/1181]\tLoss_D: 0.3815\tLoss_G: 2.4393\tD(x): 0.8580\tD(G(z)): 0.1976 / 0.0986\n",
      "[0/500][735/1181]\tLoss_D: 0.6005\tLoss_G: 2.0660\tD(x): 0.6260\tD(G(z)): 0.0704 / 0.1578\n",
      "[0/500][750/1181]\tLoss_D: 0.8581\tLoss_G: 1.3271\tD(x): 0.5246\tD(G(z)): 0.1299 / 0.2716\n",
      "[0/500][765/1181]\tLoss_D: 0.3497\tLoss_G: 2.4564\tD(x): 0.8524\tD(G(z)): 0.1637 / 0.1064\n",
      "[0/500][780/1181]\tLoss_D: 0.6089\tLoss_G: 2.1398\tD(x): 0.8012\tD(G(z)): 0.2872 / 0.1250\n",
      "[0/500][795/1181]\tLoss_D: 0.2756\tLoss_G: 2.8170\tD(x): 0.7994\tD(G(z)): 0.0438 / 0.0660\n",
      "[0/500][810/1181]\tLoss_D: 0.2236\tLoss_G: 3.5250\tD(x): 0.9411\tD(G(z)): 0.1483 / 0.0310\n",
      "[0/500][825/1181]\tLoss_D: 1.1295\tLoss_G: 1.1011\tD(x): 0.3634\tD(G(z)): 0.0603 / 0.3697\n",
      "[0/500][840/1181]\tLoss_D: 0.0787\tLoss_G: 5.2551\tD(x): 0.9567\tD(G(z)): 0.0332 / 0.0086\n",
      "[0/500][855/1181]\tLoss_D: 1.2068\tLoss_G: 1.6048\tD(x): 0.4820\tD(G(z)): 0.1803 / 0.2234\n",
      "[0/500][870/1181]\tLoss_D: 0.1629\tLoss_G: 3.2331\tD(x): 0.9798\tD(G(z)): 0.1313 / 0.0403\n",
      "[0/500][885/1181]\tLoss_D: 0.3265\tLoss_G: 2.4518\tD(x): 0.9167\tD(G(z)): 0.2021 / 0.0921\n",
      "[0/500][900/1181]\tLoss_D: 0.6529\tLoss_G: 2.5912\tD(x): 0.6195\tD(G(z)): 0.1348 / 0.0949\n",
      "[0/500][915/1181]\tLoss_D: 1.1409\tLoss_G: 2.3540\tD(x): 0.5300\tD(G(z)): 0.0460 / 0.1509\n",
      "[0/500][930/1181]\tLoss_D: 0.4614\tLoss_G: 2.8013\tD(x): 0.8079\tD(G(z)): 0.1761 / 0.0848\n",
      "[0/500][945/1181]\tLoss_D: 1.2007\tLoss_G: 0.8362\tD(x): 0.4726\tD(G(z)): 0.1994 / 0.4518\n",
      "[0/500][960/1181]\tLoss_D: 0.9779\tLoss_G: 2.0867\tD(x): 0.6417\tD(G(z)): 0.2602 / 0.1404\n",
      "[0/500][975/1181]\tLoss_D: 0.5407\tLoss_G: 3.2478\tD(x): 0.8471\tD(G(z)): 0.2667 / 0.0539\n",
      "[0/500][990/1181]\tLoss_D: 0.4526\tLoss_G: 4.3800\tD(x): 0.9534\tD(G(z)): 0.3262 / 0.0130\n",
      "[0/500][1005/1181]\tLoss_D: 0.2623\tLoss_G: 2.8187\tD(x): 0.8520\tD(G(z)): 0.0872 / 0.0631\n",
      "[0/500][1020/1181]\tLoss_D: 0.5987\tLoss_G: 1.9113\tD(x): 0.6772\tD(G(z)): 0.1190 / 0.1594\n",
      "[0/500][1035/1181]\tLoss_D: 0.4528\tLoss_G: 2.5892\tD(x): 0.7099\tD(G(z)): 0.0631 / 0.0828\n",
      "[0/500][1050/1181]\tLoss_D: 0.8010\tLoss_G: 3.1975\tD(x): 0.5629\tD(G(z)): 0.0742 / 0.0707\n",
      "[0/500][1065/1181]\tLoss_D: 0.1766\tLoss_G: 3.0704\tD(x): 0.9847\tD(G(z)): 0.1462 / 0.0517\n",
      "[0/500][1080/1181]\tLoss_D: 0.6066\tLoss_G: 3.2170\tD(x): 0.9711\tD(G(z)): 0.4216 / 0.0424\n",
      "[0/500][1095/1181]\tLoss_D: 0.1970\tLoss_G: 3.8247\tD(x): 0.9240\tD(G(z)): 0.1066 / 0.0258\n",
      "[0/500][1110/1181]\tLoss_D: 0.6773\tLoss_G: 3.0505\tD(x): 0.7422\tD(G(z)): 0.2178 / 0.0528\n",
      "[0/500][1125/1181]\tLoss_D: 0.9207\tLoss_G: 2.1554\tD(x): 0.6073\tD(G(z)): 0.2495 / 0.1348\n",
      "[0/500][1140/1181]\tLoss_D: 0.1169\tLoss_G: 3.4916\tD(x): 0.9556\tD(G(z)): 0.0677 / 0.0341\n",
      "[0/500][1155/1181]\tLoss_D: 1.2738\tLoss_G: 3.1513\tD(x): 0.4997\tD(G(z)): 0.2485 / 0.0500\n",
      "[0/500][1170/1181]\tLoss_D: 0.3769\tLoss_G: 3.1946\tD(x): 0.9696\tD(G(z)): 0.2896 / 0.0413\n",
      "epoch: 0 \t 1180/1181 batch\r"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/500][0/1181]\tLoss_D: 1.6924\tLoss_G: 0.8933\tD(x): 0.4121\tD(G(z)): 0.2219 / 0.4368\n",
      "[1/500][15/1181]\tLoss_D: 0.2660\tLoss_G: 3.0529\tD(x): 0.8344\tD(G(z)): 0.0488 / 0.0508\n",
      "[1/500][30/1181]\tLoss_D: 0.2327\tLoss_G: 3.1513\tD(x): 0.9527\tD(G(z)): 0.1590 / 0.0520\n",
      "[1/500][45/1181]\tLoss_D: 0.7546\tLoss_G: 2.9592\tD(x): 0.9387\tD(G(z)): 0.4289 / 0.0618\n",
      "[1/500][60/1181]\tLoss_D: 0.7396\tLoss_G: 2.7796\tD(x): 0.7631\tD(G(z)): 0.3110 / 0.0783\n",
      "[1/500][75/1181]\tLoss_D: 0.3545\tLoss_G: 3.2069\tD(x): 0.9964\tD(G(z)): 0.2922 / 0.0431\n",
      "epoch: 1 \t 77/1181 batch\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 74\u001B[0m\n\u001B[1;32m     72\u001B[0m errG \u001B[38;5;241m=\u001B[39m criterion(output, label)\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# Calculate gradients for G\u001B[39;00m\n\u001B[0;32m---> 74\u001B[0m \u001B[43merrG\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m D_G_z2 \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m# Update G\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "batch_size = 4\n",
    "num_epochs = 500\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    batch_count = int(np.ceil(sample_count / batch_size))\n",
    "    permutation = np.random.permutation(sample_count)\n",
    "    for batch_idx in range(batch_count):\n",
    "        print(\"epoch: {} \\t {}/{} batch\".format(epoch, batch_idx, batch_count), end=\"\\r\")\n",
    "        \n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = min(sample_count, (batch_idx + 1) * batch_size)\n",
    "        batch_indeces = permutation[batch_start:batch_end]\n",
    "        real_batch_size = batch_end - batch_start\n",
    "        \n",
    "        data = dataset_t[batch_indeces]\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data\n",
    "        label = torch.full((real_batch_size,), real_label, dtype=torch.float)\n",
    "        if USE_CUDA:\n",
    "            label = label.cuda()\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(real_batch_size, nz, noise_h, noise_w)\n",
    "        if USE_CUDA:\n",
    "            noise = noise.cuda()\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        \n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if batch_idx % 15 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, batch_idx, batch_count, errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (batch_idx == batch_count - 1 ):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(fake.numpy())\n",
    "            index = 0\n",
    "            img = fake.numpy()[0]\n",
    "            img = np.moveaxis(img, 0, -1)\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "            \n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4521732d-081d-468f-9052-65d87e373b91",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m \u001B[43mimg_list\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m150\u001B[39;49m\u001B[43m]\u001B[49m:\n\u001B[1;32m      2\u001B[0m     img \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmoveaxis(img, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      3\u001B[0m     img \u001B[38;5;241m=\u001B[39m (img \u001B[38;5;241m-\u001B[39m img\u001B[38;5;241m.\u001B[39mmin()) \u001B[38;5;241m/\u001B[39m (img\u001B[38;5;241m.\u001B[39mmax() \u001B[38;5;241m-\u001B[39m img\u001B[38;5;241m.\u001B[39mmin())\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for img in img_list[150]:\n",
    "    img = np.moveaxis(img, 0, -1)\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    \n",
    "    print(img.shape)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb2e57-3666-4b33-8ca6-47abfed094e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
